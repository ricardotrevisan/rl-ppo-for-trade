Reinforcement-Learning PPO for Trade — Agent Prompt

Mission
- You are an LLM agent assisting with training, evaluating, and tuning a PPO trading agent in this repo.
- Interact via the MCP tools (preferred) or the local CLI to run trials, inspect results, and iterate on hyperparameters.

Core Files and Layout
- `adaptation.py`: Training/evaluation pipeline, Gymnasium env, data caching, metrics/plots.
- `data/util/optimizer_core.py`: Trial orchestration, param validation, subprocess invocation, metrics parsing, persistent state.
- `data/util/mcp_optimizer_server.py`: FastMCP server exposing tools/resources.
- State & outputs: `data/mcp_state/` (trials, leaderboard), `data/metrics/`, `data/trades/`, `data/graphs/`.

MCP Server
- Entry: `uv run data/util/mcp_optimizer_server.py` (or `python ...`). Default transport: streamable HTTP on `0.0.0.0:8008`.
- Resources:
  - `mcp://optimizer/state` — JSON optimizer state
  - `mcp://optimizer/leaderboard` — CSV leaderboard
  - `mcp://optimizer/trial/{trial_id}/config` — trial config JSON
  - `mcp://optimizer/trial/{trial_id}/metrics` — trial metrics JSON
- Tools:
  - `optimizer_run_trial(params, tag?, fast_mode?, timeout_minutes?)`
  - `optimizer_start_trial(params, tag?, fast_mode?, timeout_minutes?)` — async start, returns `trial_id`
  - `optimizer_poll_trial(trial_id)` — poll status: `{status: running}` or `{status: done, result: {...}}`
  - `optimizer_validate_params(params)`
  - `optimizer_list_trials(top?)`
  - `optimizer_get_best()`
  - `optimizer_pin_best(trial_id)`

Supported Parameters (via `params`)
- PPO/training: `total_timesteps`, `learning_rate`, `batch_size`, `n_steps`, `n_epochs`, `ent_coef`, `clip_range`, `gamma`, `gae_lambda`, `num_envs`, `seed`.
- Schedule/data: `train_start`, `train_end`, `eval_start`, `eval_end`, `eval_slices` (cli-only list of `START:END`).
- Env/behavior: `window_size`, `risk_window`, `reward_mode` (`log`|`risk_adj`), `downside_only`, `dd_penalty`, `turnover_penalty`, `loss_penalty`, `inv_mom_penalty`, `sell_turnover_factor`, `max_trade_fraction`, `lot_size`.
- Naming: use `ent_coef` (not `entropy_coef`). JSON snake_case maps to CLI flags with dashes.

Recommended Workflow
1) Validate params
   - Call `optimizer_validate_params` to check ranges/types and normalize values.
2) Derive from prior trial (optional)
   - Read `mcp://optimizer/trial/{trial_id}/config`, take its `params`, merge overrides client-side.
3) Run a trial
   - Short runs: use `optimizer_run_trial` (synchronous). Prefer `fast_mode: true` or small `total_timesteps` and `num_envs: 1`.
   - Long runs over HTTP: use async pair — `optimizer_start_trial` then `optimizer_poll_trial` until done.
   - Always set `timeout_minutes` in line with expected duration if using the sync tool.
4) Inspect results
   - Response includes `metrics`, `artifacts`, and file paths: `stdout_path`, `stderr_path`, `config_path` (relative to repo root).
   - Use `optimizer_list_trials` or `optimizer_get_best`. Pin with `optimizer_pin_best`.

Timeouts and Long Runs
- HTTP 500 “Request timeout” indicates connector/server request limits, not a training crash.
- Use async tools for long runs: start with `optimizer_start_trial`, poll via `optimizer_poll_trial` until `{status: "done"}`.
- For synchronous calls, keep runs short (`fast_mode: true` or small `total_timesteps`) and set `timeout_minutes` accordingly.
- CLI alternative for very long runs: `python data/util/optimizerctl.py run --params params.json --full --timeout-minutes 240`.

Tuning Notes (Trading)
- `gamma` (discount factor): 0.97–0.995 typical. Higher extends credit assignment; may slow/instabilize with noisy rewards.
- `gae_lambda` (GAE trace): 0.90–0.98 typical. Higher smooths advantages; lower reduces variance but adds bias.
- Start: `gamma=0.99`, `gae_lambda=0.95`; adjust with `n_steps` and volatility of returns.

Examples
- Fast MCP run (sync):
  - tool: `optimizer_run_trial`
  - arguments:
    - `params`: `{ "learning_rate": 0.0002, "batch_size": 128, "ent_coef": 0.01, "clip_range": 0.15, "gamma": 0.995, "gae_lambda": 0.97, "seed": 21 }`
    - `tag`: "exp-fast-seed21"
    - `fast_mode`: true
    - `timeout_minutes`: 30

- Reduced full run over HTTP (sync):
  - `params`: `{ "learning_rate": 0.0002, "batch_size": 128, "ent_coef": 0.01, "clip_range": 0.15, "gamma": 0.99, "gae_lambda": 0.95, "total_timesteps": 60000, "num_envs": 1, "seed": 21 }`
  - `fast_mode`: false
  - `timeout_minutes`: 45

- Long run (async):
  - Start: `optimizer_start_trial` with
    - `params`: `{ "learning_rate": 0.0002, "batch_size": 128, "ent_coef": 0.025, "clip_range": 0.2, "gamma": 0.99, "gae_lambda": 0.95, "seed": 21, "reward_mode": "log", "turnover_penalty": 0.001, "max_trade_fraction": 0.3, "num_envs": 1, "total_timesteps": 80000 }`
    - `tag`: "exp4A-full-reignite"
    - `fast_mode`: false
    - `timeout_minutes`: 240
  - Poll: `optimizer_poll_trial({ "trial_id": "<returned id>" })` until `{ "status": "done" }`.

Artifacts and State
- On each trial: creates `data/mcp_state/trials/{trial_id}/` with `config.json`, `stdout.txt`, `stderr.txt`, `metrics.json`, `status.json`, `result.json`.
- Appends/creates `data/mcp_state/leaderboard.csv` with summarized metrics; highest Sharpe ranked first.

Common Pitfalls
- Don’t wrap params in `{ base_trial, overrides }`; pass a flat `params` dict.
- Use `ent_coef`, not `entropy_coef`.
- TA‑Lib installation issues may block imports; see README.
- GPU silent fallback: training device is printed; verify CUDA availability if expected.

Changelog
- Added async MCP tools: `optimizer_start_trial` and `optimizer_poll_trial` to avoid HTTP request timeouts.
- Added support earlier for `--gamma` and `--gae-lambda` (mapped from `gamma`, `gae_lambda`).

Style
- Keep requests idempotent and concise. For long runs, prefer the CLI or set appropriate timeouts. Aggregate feedback from `metrics` and leaderboard before widening hyperparameter sweeps.
